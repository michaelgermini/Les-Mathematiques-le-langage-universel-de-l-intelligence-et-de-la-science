# 4.4. Applications : compression, réseaux neuronaux, graphes

## Introduction : L'algèbre linéaire au cœur de la révolution numérique

L'algèbre linéaire ne se contente pas d'être une théorie abstraite ; elle constitue le moteur invisible derrière la plupart des technologies modernes. De la compression d'images aux réseaux neuronaux, en passant par l'analyse de graphes, ses applications transforment notre quotidien.

## Compression de données : SVD et PCA

### Compression d'images par SVD

#### Principe mathématique
Une image en niveaux de gris : matrice A de taille m×n
- **SVD** : A = U Σ V*
- **Approximation** : Aₖ = Uₖ Σₖ Vₖ* (rang k)
- **Erreur** : ||A - Aₖ||₂ = σ_{k+1}

#### Avantages de la SVD
- **Compression optimale** : erreur quadratique minimale pour un rang donné
- **Décomposition hiérarchique** : importance décroissante des composantes
- **Robustesse** : fonctionne sur tout type de matrice

#### Exemple numérique
Image 512×512 pixels :
- **SVD complète** : 512×512×2 ≈ 500 Ko (coefficients)
- **Rang 50** : 50×(512+512) ≈ 50 Ko (compression 10×)
- **Qualité** : PSNR > 30 dB (quasi-indistinguible)

### Analyse en composantes principales (PCA)

#### Algorithme
1. **Centrage** : X ← X - mean(X)
2. **Matrice de covariance** : C = X*X / n
3. **SVD** : C = V Σ² V*
4. **Projection** : nouvelles coordonnées Y = X Vₖ

#### Applications biologiques
- **Génomique** : analyse de populations génétiques
- **Transcriptomique** : expression de gènes
- **Imagerie médicale** : réduction de dimension

#### Visualisation de données
- **t-SNE vs PCA** : PCA linéaire, t-SNE non-linéaire
- **Clustering** : groupes naturels dans les données
- **Détection d'anomalies** : points éloignés des composantes principales

## Réseaux neuronaux : l'algèbre linéaire incarnée

### Neurones artificiels et transformations linéaires

#### Perceptron simple
Un neurone : y = σ(w·x + b)
- **w** : vecteur des poids (espace d'entrée)
- **x** : vecteur d'entrée
- **σ** : fonction d'activation (non-linéaire)

#### Couches cachées
Une couche : Y = σ(W X + b)
- **W** : matrice des poids (transformation linéaire)
- **X** : matrice des activations d'entrée
- **b** : vecteur de biais

### Backpropagation et calcul de gradients

#### Chaîne de dérivation
Erreur quadratique : L = ||ŷ - y||²
- **Dérivée par rapport aux poids** : ∂L/∂W
- **Règle de la chaîne** : propagation arrière des gradients
- **Descente de gradient** : W ← W - η ∂L/∂W

#### Algèbre matricielle
- **Produit matriciel** : calcul efficace des activations
- **Transposition** : pour la backpropagation
- **Broadcasting** : calcul vectorisé

### Architectures avancées

#### Réseaux convolutifs (CNN)
- **Filtres** : matrices de convolution
- **Pooling** : réduction de dimension
- **Invariance** : translation, rotation

#### Transformers
- **Attention** : produits scalaires pour les poids d'attention
- **Embeddings** : espaces vectoriels de représentation
- **Matrices d'attention** : softmax(QK*)V

### Optimisation et régularisation

#### Descente de gradient stochastique
- **SGD** : estimation des gradients sur mini-batch
- **Adam** : adaptation du pas d'apprentissage
- **Momentum** : accélération dans les directions pertinentes

#### Régularisation
- **L2** : ||W||₂² (pénalisation des grands poids)
- **Dropout** : régularisation stochastique
- **Batch normalization** : normalisation des activations

## Analyse de graphes : matrices d'adjacence

### Représentation matricielle des graphes

#### Matrice d'adjacence
Graphe G = (V,E) :
- **Aᵢⱼ = 1** si arête (i,j), 0 sinon
- **Puissances** : Aᵏ compte les chemins de longueur k
- **Spectre** : valeurs propres de A

#### Laplacien du graphe
L = D - A où D est diagonale des degrés
- **Propriétés** : semi-définie positive
- **Valeur propre nulle** : vecteur tout-un
- **Connexité** : nombre de composantes connexes

### Algorithmes basés sur l'algèbre linéaire

#### PageRank (Google)
- **Matrice stochastique** : Mᵢⱼ = probabilité de i vers j
- **Vecteur stationnaire** : π M = π
- **Score de centralité** : importance des pages

#### Clustering spectral
1. **Laplacien normalisé** : L_norm = D^{-1/2} L D^{-1/2}
2. **Vecteurs propres** : premiers vecteurs propres non triviaux
3. **k-means** : sur les coordonnées spectrales

#### Détection de communautés
- **Modularité** : mesure de la qualité du partitionnement
- **Optimisation** : algorithmes spectraux
- **Applications** : réseaux sociaux, biologie

### Réseaux complexes

#### Small-world et scale-free
- **Diamètre** : log n pour réseaux scale-free
- **Clustering** : coefficient de clustering élevé
- **Centralité** : mesures spectrales

#### Dynamique sur réseaux
- **Épidémies** : seuil épidémique spectral
- **Consensus** : algorithmes de moyenne consensuelle
- **Synchronisation** : matrice de couplage

## Applications interdisciplinaires

### Finance quantitative

#### Modèles de risque
- **VaR (Value at Risk)** : risque de perte maximale
- **Matrices de covariance** : volatilité des actifs
- **Optimisation de portefeuille** : Markowitz

#### Pricing d'options
- **Black-Scholes** : équation aux dérivées partielles
- **Simulation Monte-Carlo** : génération de scénarios
- **Méthodes numériques** : différences finies

### Physique et ingénierie

#### Mécanique des structures
- **Matrices de rigidité** : analyse par éléments finis
- **Modes propres** : vibrations et stabilité
- **Optimisation topologique** : conception légère

#### Traitement du signal
- **FFT (Fast Fourier Transform)** : analyse fréquentielle
- **Filtres numériques** : convolution circulaire
- **Compression audio/vidéo** : codage prédictif

### Sciences sociales

#### Analyse de réseaux sociaux
- **Centralité eigenvector** : influence dans les réseaux
- **Diffusion d'information** : modèles épidémiques
- **Recommandation** : factorisation matricielle

#### Économie
- **Input-output** : modèle Leontief
- **Équilibres généraux** : systèmes d'équations
- **Théorie des jeux** : matrices de payoff

## Défis computationnels et perspectives

### Évolutivité

#### Big Data
- **Matrices creuses** : stockage efficace (CSR, CSC)
- **Algorithmes distribués** : MapReduce, Spark
- **Approximation** : méthodes randomisées

#### Deep Learning à grande échelle
- **GPU computing** : calcul matriciel parallèle
- **Mixed precision** : optimisation mémoire
- **Quantization** : réduction de précision

### Nouvelles architectures

#### Réseaux neuromorphiques
- **Calculs locaux** : réduction de la complexité
- **Apprentissage en ligne** : adaptation continue
- **Efficacité énergétique** : inspiration biologique

#### Quantum computing
- **Algorithmes quantiques** : Grover, HHL
- **Simulation quantique** : algorithme de Feynman
- **Optimisation** : QAOA (Quantum Approximate Optimization Algorithm)

## Conclusion : L'algèbre linéaire comme langage universel

Des plus humbles compressions d'images aux architectures d'IA les plus sophistiquées, l'algèbre linéaire constitue le langage commun qui relie les disciplines et les applications. Sa puissance réside dans sa capacité à révéler les structures cachées des données et à permettre des transformations efficaces dans tous les domaines de la science et de la technologie.

Dans l'ère du numérique, maîtriser l'algèbre linéaire n'est plus un luxe mathématique – c'est une nécessité pour comprendre et façonner le monde moderne.

