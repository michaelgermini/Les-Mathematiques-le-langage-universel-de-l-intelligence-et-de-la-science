# 6.1. Probabilité : axiomes et intuition

## Introduction : Mesurer l'incertitude

La probabilité est la branche des mathématiques qui quantifie l'incertitude et le hasard. De la roulette de casino aux phénomènes quantiques, elle fournit un cadre rigoureux pour raisonner sur les événements aléatoires et faire des prédictions dans un monde incertain.

## Les origines historiques

### Jeux de hasard (XVIIe siècle)

#### Problème des points (Pascal, 1654)
Deux joueurs interrompent une partie :
- **Question** : comment répartir l'enjeu équitablement ?
- **Solution** : calcul des probabilités de gagner

#### Correspondance Pascal-Fermat
- **Fondements** : première formalisation mathématique
- **Combinatoire** : arrangements et combinaisons
- **Équiprobabilité** : hypothèse d'équiprobabilité

### Théorie classique (Laplace, 1812)

#### Définition
P(A) = (nombre de cas favorables) / (nombre de cas possibles)

- **Hypothèse** : tous les cas équiprobables
- **Limites** : ne s'applique pas à tous les phénomènes
- **Succès** : mécanique statistique, thermodynamique

## Axiomatique moderne (Kolmogorov, 1933)

### Espace probabilisé

Un espace probabilisé (Ω, ℱ, P) comprend :
- **Ω** : ensemble des résultats possibles (univers)
- **ℱ** : tribu (σ-algèbre) d'événements
- **P** : mesure de probabilité

### Axiomes de Kolmogorov

1. **Positivité** : P(A) ≥ 0 pour tout A ∈ ℱ
2. **Normalisation** : P(Ω) = 1
3. **Additivité dénombrable** : P(∪ A_n) = Σ P(A_n) pour événements disjoints

### Propriétés dérivées

#### Complémentaire
P(Aᶜ) = 1 - P(A)

#### Union
P(A ∪ B) = P(A) + P(B) - P(A ∩ B)

#### Inclusion
Si A ⊂ B alors P(A) ≤ P(B)

## Probabilités conditionnelles

### Définition
P(A|B) = P(A ∩ B) / P(B) si P(B) > 0

- **Interprétation** : probabilité de A sachant B
- **Formule des probabilités composées** : P(A ∩ B) = P(A|B) P(B)

### Règle de Bayes

P(B|A) = [P(A|B) P(B)] / P(A)

- **Probabilité a priori** : P(B) avant observation
- **Vraisemblance** : P(A|B) probabilité des données sachant l'hypothèse
- **Probabilité a posteriori** : P(B|A) après observation

## Variables aléatoires

### Définition
Une variable aléatoire X : Ω → ℝ associe une valeur numérique à chaque résultat

### Fonction de répartition
F_X(x) = P(X ≤ x)

- **Propriétés** : croissante, continue à droite, F(-∞) = 0, F(∞) = 1
- **Continuité** : P(X = x) = 0 pour variables continues

### Espérance mathématique
E[X] = ∫ x dF_X(x)

- **Propriétés** : linéarité, positivité pour X ≥ 0
- **Interprétation** : valeur moyenne pondérée

### Variance
Var(X) = E[(X - E[X])²] = E[X²] - (E[X])²

- **Unité** : carré de l'unité de X
- **Écart-type** : σ = √Var(X)
- **Coefficient de variation** : σ/μ

## Distributions importantes

### Distributions discrètes

#### Loi uniforme discrète
P(X = k) = 1/n pour k = 1,2,...,n

#### Loi de Bernoulli
P(X = 1) = p, P(X = 0) = 1-p

#### Loi binomiale
P(X = k) = C(n,k) p^k (1-p)^{n-k}

#### Loi de Poisson
P(X = k) = e^{-λ} λ^k / k!

### Distributions continues

#### Loi uniforme continue
f(x) = 1/(b-a) sur [a,b]

#### Loi exponentielle
f(x) = λ e^{-λx} pour x > 0

#### Loi normale (Gaussienne)
f(x) = (1/√(2π σ²)) exp(-(x-μ)²/(2σ²))

## Lois des grands nombres

### Loi faible des grands nombres
Pour des v.a. i.i.d. avec E[|X_i|] < ∞ :
(1/n) Σ X_i → E[X] en probabilité

### Loi forte des grands nombres
(1/n) Σ X_i → E[X] presque sûrement

### Théorème central limite
Pour des v.a. i.i.d. avec variance finie :
√n (Ȳ_n - μ) / σ → N(0,1) en loi

## Indépendance et corrélation

### Indépendance
X et Y indépendantes si P(X ∈ A, Y ∈ B) = P(X ∈ A) P(Y ∈ B)

- **Équivalent** : F_{X,Y}(x,y) = F_X(x) F_Y(y)
- **Conséquence** : E[XY] = E[X] E[Y]

### Covariance
Cov(X,Y) = E[(X - μ_X)(Y - μ_Y)]

- **Propriétés** : bilinéaire, symétrique
- **Lien avec l'indépendance** : Cov = 0 si indépendantes (mais réciproque fausse)

### Coefficient de corrélation
ρ = Cov(X,Y) / (σ_X σ_Y)

- **Bornes** : -1 ≤ ρ ≤ 1
- **Interprétation** : force et direction de la liaison linéaire

## Applications intuitives

### Paradoxes et illusions

#### Paradoxe de Monty Hall
- **Problème** : changer de porte augmente-t-il les chances ?
- **Réponse** : oui, de 1/3 à 2/3
- **Contre-intuition** : probabilités conditionnelles

#### Problème des deux enveloppes
- **Situation** : choisir l'enveloppe avec plus d'argent
- **Paradoxe** : argument suggère toujours changer
- **Résolution** : espérances infinies impossibles

### Raisonnement probabiliste quotidien

#### Tests médicaux
- **Sensibilité** : P(test+|maladie)
- **Spécificité** : P(test-|santé)
- **Valeur prédictive** : P(maladie|test+)

#### Justice et erreur judiciaire
- **Probabilité d'erreur** : type I et type II
- **Test d'ADN** : calcul de vraisemblance

## Philosophie des probabilités

### Interprétations

#### Fréquentiste
- **Probabilité** : limite de fréquences relatives
- **Objectivité** : indépendante de l'observateur
- **Limites** : événements uniques, probabilités subjectives

#### Subjectiviste (Bayésien)
- **Probabilité** : degré de croyance rationnel
- **Mise à jour** : théorème de Bayes
- **Avantages** : traitement des incertitudes

#### Propension
- **Probabilité** : propriété physique des mécanismes
- **Exemple** : probabilité d'une pièce de se stabiliser sur pile

### Problèmes fondamentaux

#### Problème de la mesure
Comment assigner des probabilités à des événements complexes ?

#### Induction
Comment généraliser des observations finies à des lois générales ?

#### Causalité vs corrélation
La corrélation implique-t-elle la causalité ?

## Conclusion : La probabilité comme langage de l'incertitude

Les axiomes de Kolmogorov et l'intuition probabiliste forment un cadre cohérent pour quantifier l'incertitude. De la roulette au climat, de la physique quantique aux marchés financiers, la probabilité permet de raisonner rigoureusement sur un monde fondamentalement aléatoire.

Dans l'ère de l'IA et du Big Data, maîtriser le raisonnement probabiliste devient essentiel pour comprendre et naviguer dans un monde incertain.

