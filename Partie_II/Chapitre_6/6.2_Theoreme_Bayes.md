# 6.2. Théorème de Bayes

## Introduction : L'inférence probabiliste

Le théorème de Bayes, formulé par Thomas Bayes au XVIIIe siècle, constitue le fondement de l'inférence statistique moderne. Il permet de mettre à jour nos croyances à la lumière de nouvelles données, formant la base de l'approche bayésienne en statistique et en intelligence artificielle.

## Formulation mathématique

### Théorème de Bayes

P(H|D) = [P(D|H) P(H)] / P(D)

Où :
- **H** : hypothèse (hypothesis)
- **D** : données (data)
- **P(H)** : probabilité a priori de H
- **P(D|H)** : vraisemblance (likelihood)
- **P(H|D)** : probabilité a posteriori
- **P(D)** : probabilité marginale des données

### Interprétation

- **P(H)** : croyance initiale avant observation
- **P(D|H)** : plausibilité des données sous l'hypothèse
- **P(H|D)** : croyance mise à jour après observation
- **P(D)** : normalisation (évite les probabilités non-normalisées)

## Applications médicales

### Test diagnostique

Considérons un test de dépistage :
- **Prévalence** : P(malade) = 0.01 (1% de la population)
- **Sensibilité** : P(test+|malade) = 0.99
- **Spécificité** : P(test-|sain) = 0.95

**Probabilité d'être malade sachant test positif :**
P(malade|test+) = [0.99 × 0.01] / P(test+)

Où P(test+) = P(test+|malade)P(malade) + P(test+|sain)P(sain)
= 0.99×0.01 + 0.05×0.99 = 0.0099 + 0.0495 = 0.0594

Donc P(malade|test+) ≈ 0.0099 / 0.0594 ≈ 0.167 (16.7%)

### Interprétation
Malgré un test très sensible et spécifique, la probabilité postérieure reste faible à cause de la faible prévalence.

## Apprentissage bayésien

### Classification naïve bayésienne

Pour classer un document en catégories :
P(catégorie|mot₁, mot₂, ..., motₙ) ∝ P(mot₁|catégorie) × ... × P(motₙ|catégorie) × P(catégorie)

- **Hypothèse d'indépendance** : mots indépendants conditionnellement
- **Efficace** : malgré l'approximation, excellent pour le spam

### Réseaux bayésiens

- **Graphes** : variables comme nœuds, dépendances comme arêtes
- **Inférence** : propagation des probabilités
- **Applications** : diagnostic médical, systèmes experts

## Inférence statistique bayésienne

### Estimation de paramètres

Pour estimer θ connaissant des données D :
P(θ|D) ∝ P(D|θ) × P(θ)

- **Prior** : P(θ) connaissance a priori
- **Likelihood** : P(D|θ) probabilité des données
- **Posterior** : distribution mise à jour

### Exemple : pièce biaisée

- **Prior** : Beta(α,β) pour la probabilité p de pile
- **Likelihood** : Binomiale(n,k) pour k piles sur n lancers
- **Posterior** : Beta(α+k, β+n-k)

## Philosophie bayésienne

### Subjectivisme rationnel

- **Probabilités** : degrés de croyance personnels
- **Cohérence** : obéissance aux axiomes de probabilité
- **Mise à jour** : incorporation de nouvelles informations

### Avantages

- **Traitement naturel de l'incertitude**
- **Incorporation de connaissances a priori**
- **Quantification explicite de l'incertitude**

### Critique fréquentiste

- **Subjectivité** : priors dépendent du chercheur
- **Complexité computationnelle** (historique)
- **Objectivité** : approche fréquentiste plus "scientifique"

## Applications modernes

### Intelligence artificielle

#### Apprentissage automatique bayésien
- **Réseaux bayésiens** : apprentissage de structures
- **Modèles génératifs** : VAE, GAN avec approche bayésienne
- **Optimisation bayésienne** : recherche d'hyperparamètres

#### Robotique
- **Localisation** : filtre de Kalman, particules
- **Planification** : prise de décision sous incertitude
- **Apprentissage par renforcement** : méthodes bayésiennes

### Sciences des données

#### A/B testing
- **Test de conversion** : estimation de l'effet
- **Bandits multi-bras** : optimisation séquentielle
- **Personnalisation** : recommandation bayésienne

#### Analyse de risque
- **Finance** : stress testing, Value at Risk
- **Assurance** : tarification bayésienne
- **Épidémiologie** : estimation de paramètres

## Extensions et généralisations

### Théorème de Bayes étendu

Pour plusieurs hypothèses :
P(H_i|D) ∝ P(D|H_i) P(H_i)

- **Normalisation** : diviser par Σ P(D|H_j) P(H_j)
- **Maximum a posteriori** : argmax P(H_i|D)

### Inférence hiérarchique

- **Modèles à niveaux** : paramètres avec leurs propres priors
- **Partage d'information** : entre groupes similaires
- **Applications** : méta-analyse, classification multi-niveaux

## Défis computationnels

### Intégrales complexes

Le calcul de P(D) = ∫ P(D|θ) P(θ) dθ nécessite souvent :
- **Monte-Carlo** : échantillonnage par chaînes de Markov (MCMC)
- **Variational inference** : approximation par familles simples
- **Méthodes déterministes** : quadrature numérique

### Big Data
- **Approximations** : mini-batch, streaming
- **Scalabilité** : méthodes distribuées
- **Online learning** : mise à jour incrémentale

## Conclusion : Le raisonnement bayésien comme méthode scientifique

Le théorème de Bayes offre un cadre élégant pour intégrer de nouvelles informations à nos connaissances existantes. Au-delà de sa rigueur mathématique, il représente une philosophie de l'apprentissage continu et de l'adaptation rationnelle face à l'incertitude.

Dans l'ère des données massives et de l'IA, l'approche bayésienne retrouve une place centrale, permettant de quantifier l'incertitude et de prendre des décisions robustes dans un monde complexe.

