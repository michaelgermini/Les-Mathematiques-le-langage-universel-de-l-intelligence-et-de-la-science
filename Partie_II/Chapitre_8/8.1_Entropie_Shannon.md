# 8.1. Entropie de Shannon

## Introduction

En 1948, Claude Shannon publie "A Mathematical Theory of Communication", fondant la théorie de l'information. Au cœur de cette théorie se trouve l'**entropie**, une mesure mathématique de l'incertitude et de la quantité d'information. Ce concept révolutionnaire unifie communication, compression et même physique statistique.

---

## 1. Information et surprise

### 1.1 Quantifier l'information

Intuitivement, un événement improbable est plus "informatif" qu'un événement prévisible. Shannon formalise cette intuition.

#### Information d'un événement
Pour un événement de probabilité p, son **contenu informationnel** est :
```
I(p) = -log₂(p) bits
```

#### Propriétés
| Probabilité | Information | Interprétation |
|-------------|-------------|----------------|
| p = 1 | I = 0 bits | Événement certain, aucune surprise |
| p = 0.5 | I = 1 bit | Pile ou face |
| p = 0.25 | I = 2 bits | Une carte parmi 4 |
| p = 0.01 | I ≈ 6.6 bits | Événement rare, très informatif |

#### Justification axiomatique
Shannon montre que I(p) est l'unique fonction (à constante près) satisfaisant :
1. **Continuité** : I varie continûment avec p
2. **Monotonicité** : plus p est petit, plus I est grand
3. **Additivité** : I(p₁p₂) = I(p₁) + I(p₂) pour événements indépendants

### 1.2 Unités d'information

| Base du logarithme | Unité | Usage |
|--------------------|-------|-------|
| 2 | **bit** | Informatique, communication |
| e | **nat** | Théorie, physique |
| 10 | **dit** ou **hartley** | Historique |

Conversion : 1 nat = log₂(e) ≈ 1.443 bits

---

## 2. Entropie d'une variable aléatoire

### 2.1 Définition

L'**entropie** H(X) d'une variable aléatoire discrète X est l'information moyenne :
```
H(X) = E[I(X)] = -Σₓ P(x) log₂ P(x)
```

avec la convention 0 log 0 = 0 (limite).

### 2.2 Exemples fondamentaux

#### Pièce biaisée
Pour une pièce avec P(pile) = p :
```
H(X) = -p log₂ p - (1-p) log₂(1-p)
```
Cette fonction est appelée **entropie binaire** h(p).

| p | H(X) |
|---|------|
| 0 ou 1 | 0 bit (déterministe) |
| 0.5 | 1 bit (maximum) |
| 0.1 | 0.47 bit |

#### Dé équilibré
Pour un dé à n faces équiprobables :
```
H(X) = log₂(n)
```
- Dé 6 faces : H = log₂(6) ≈ 2.58 bits
- Dé 2 faces (pièce) : H = 1 bit

### 2.3 Propriétés fondamentales

#### Positivité
```
H(X) ≥ 0
```
Égalité si et seulement si X est déterministe (une valeur de probabilité 1).

#### Borne supérieure
```
H(X) ≤ log₂(|X|)
```
où |X| est le nombre de valeurs possibles. Égalité pour la distribution uniforme.

#### Concavité
H est concave : le mélange de distributions augmente l'entropie.

---

## 3. Entropie conjointe et conditionnelle

### 3.1 Entropie conjointe

Pour deux variables X et Y :
```
H(X,Y) = -Σₓ,ᵧ P(x,y) log₂ P(x,y)
```

#### Règle de chaîne
```
H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
```

### 3.2 Entropie conditionnelle

L'incertitude sur X sachant Y :
```
H(X|Y) = Σᵧ P(y) H(X|Y=y) = -Σₓ,ᵧ P(x,y) log₂ P(x|y)
```

#### Propriété fondamentale
```
H(X|Y) ≤ H(X)
```
Connaître Y ne peut que réduire (ou maintenir) l'incertitude sur X.

Égalité si et seulement si X et Y sont indépendantes.

### 3.3 Information mutuelle

L'**information mutuelle** mesure l'information partagée entre X et Y :
```
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)
```

#### Propriétés
- **Symétrie** : I(X;Y) = I(Y;X)
- **Positivité** : I(X;Y) ≥ 0
- **Zéro si indépendantes** : I(X;Y) = 0 ⟺ X ⊥ Y
- **Borne** : I(X;Y) ≤ min(H(X), H(Y))

#### Diagramme de Venn informationnel
```
    ┌─────────────────────────────┐
    │         H(X,Y)              │
    │  ┌─────────┬─────────┐      │
    │  │  H(X|Y) │  I(X;Y) │H(Y|X)│
    │  │         │         │      │
    │  └─────────┴─────────┘      │
    │     H(X)       H(Y)         │
    └─────────────────────────────┘
```

---

## 4. Divergence de Kullback-Leibler

### 4.1 Définition

La **divergence KL** (ou entropie relative) mesure la "distance" entre deux distributions :
```
D_KL(P || Q) = Σₓ P(x) log₂(P(x)/Q(x))
```

### 4.2 Propriétés

- **Non-négativité** : D_KL(P || Q) ≥ 0 (inégalité de Gibbs)
- **Zéro** : D_KL(P || Q) = 0 ⟺ P = Q
- **Asymétrie** : D_KL(P || Q) ≠ D_KL(Q || P) en général
- **Non métrique** : ne satisfait pas l'inégalité triangulaire

### 4.3 Interprétation

D_KL(P || Q) représente le coût supplémentaire (en bits) pour encoder des données de distribution P avec un code optimal pour Q.

### 4.4 Entropie croisée

```
H(P, Q) = -Σₓ P(x) log₂ Q(x) = H(P) + D_KL(P || Q)
```

Très utilisée comme fonction de perte en machine learning (classification).

---

## 5. Théorèmes fondamentaux de Shannon

### 5.1 Premier théorème (codage source)

**Énoncé** : Une source de symboles d'entropie H peut être compressée jusqu'à H bits par symbole en moyenne, mais pas moins.

#### Implications
- L'entropie est la **limite de compression** sans perte
- Le codage de Huffman atteint cette limite asymptotiquement
- Le codage arithmétique l'atteint avec précision arbitraire

### 5.2 Codage de Huffman

Algorithme de compression optimal pour des symboles indépendants :

1. Créer un nœud feuille pour chaque symbole avec sa probabilité
2. Répéter jusqu'à un seul nœud :
   - Fusionner les deux nœuds de plus faible probabilité
3. Assigner 0/1 aux branches

#### Exemple
| Symbole | Probabilité | Code Huffman |
|---------|-------------|--------------|
| A | 0.5 | 0 |
| B | 0.25 | 10 |
| C | 0.125 | 110 |
| D | 0.125 | 111 |

Longueur moyenne : 0.5×1 + 0.25×2 + 0.125×3 + 0.125×3 = 1.75 bits
Entropie : H = 1.75 bits ✓

### 5.3 Codage arithmétique

- Encode le message entier comme un nombre dans [0,1)
- Atteint l'entropie avec précision arbitraire
- Plus efficace que Huffman pour les faibles probabilités

---

## 6. Entropie en physique

### 6.1 Connexion avec la thermodynamique

Ludwig Boltzmann et Josiah Willard Gibbs ont établi le lien entre entropie informationnelle et physique :
```
S = k_B × H
```
où k_B ≈ 1.38 × 10⁻²³ J/K est la constante de Boltzmann.

### 6.2 Interprétation statistique

L'entropie thermodynamique mesure le nombre de micro-états compatibles avec un macro-état :
```
S = k_B ln(Ω)
```
où Ω est le nombre de configurations microscopiques.

### 6.3 Deuxième principe

L'entropie d'un système isolé ne peut que croître (ou rester constante) :
```
dS/dt ≥ 0
```
Interprétation informationnelle : l'information sur l'état microscopique se perd.

### 6.4 Principe de maximum d'entropie

En l'absence d'information, la distribution la plus "honnête" est celle qui maximise l'entropie sous les contraintes connues.

| Contrainte | Distribution maximisant H |
|------------|---------------------------|
| Support fini | Uniforme |
| Moyenne fixée | Exponentielle |
| Moyenne et variance fixées | Gaussienne |

---

## 7. Entropie quantique

### 7.1 Entropie de von Neumann

Pour un système quantique décrit par une matrice densité ρ :
```
S(ρ) = -Tr(ρ log ρ)
```

### 7.2 Propriétés particulières

- **États purs** : S = 0 (connaissance complète)
- **États mixtes** : S > 0 (incertitude)
- **Intrication** : l'entropie conditionnelle peut être négative !

### 7.3 Trous noirs

L'entropie de Bekenstein-Hawking d'un trou noir :
```
S = A/(4 l_P²)
```
où A est l'aire de l'horizon et l_P la longueur de Planck.

Suggère que l'information est encodée sur la surface (principe holographique).

---

## 8. Applications modernes

### 8.1 Machine learning

- **Entropie croisée** : fonction de perte standard pour la classification
- **Gain d'information** : critère de construction des arbres de décision
- **Régularisation entropique** : soft attention, exploration en RL

### 8.2 Compression de données

- **Codage entropique** : Huffman, arithmétique, ANS
- **Prédiction + entropie** : DEFLATE (LZ77 + Huffman)
- **Compression neurale** : autoencodeurs variationnels

### 8.3 Cryptographie

- **Entropie des clés** : mesure de la force d'un mot de passe
- **Générateurs aléatoires** : extraction d'entropie
- **Chiffrement parfait** : one-time pad requiert H(clé) ≥ H(message)

---

## Conclusion

L'entropie de Shannon est l'une des idées les plus fécondes du XXe siècle :

1. **Quantifie l'information** de manière rigoureuse et universelle
2. **Établit les limites fondamentales** de la compression et de la transmission
3. **Unifie** théorie de l'information, physique statistique et inférence

De la compression JPEG à la thermodynamique des trous noirs, l'entropie est le fil rouge qui relie notre compréhension de l'information sous toutes ses formes.
