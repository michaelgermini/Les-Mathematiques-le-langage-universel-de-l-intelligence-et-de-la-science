# 10.1. Fonctionnement mathématique d'un perceptron

## Le modèle biologique

### Neurone biologique
- **Dendrites** : récepteurs de signaux entrants
- **Corps cellulaire** : intégration des signaux
- **Axone** : transmission du signal de sortie
- **Synapses** : connexions (excitatrices ou inhibitrices)

### Apprentissage de Hebb
"Neurons that fire together wire together"
- **Potentiation** : connexions renforcées par activation simultanée
- **Dépression** : connexions affaiblies par inactivité
- **Plasticité** : synapses adaptatives au cours du temps

## Le perceptron de Rosenblatt (1958)

### Modèle mathématique
y = sign(Σ w_i x_i + b)

- **Entrées** : x = (x₁, ..., x_d) vecteur de caractéristiques
- **Poids** : w = (w₁, ..., w_d) paramètres apprenables
- **Biais** : b (seuil d'activation)
- **Sortie** : {-1, +1} ou {0, 1} selon la convention

### Géométrie du perceptron
- **Hyperplan séparateur** : w·x + b = 0
- **Marge** : distance d'un point à l'hyperplan
- **Séparation linéaire** : classes séparables par un hyperplan

## Algorithme d'apprentissage

### Règle de mise à jour
w ← w + η (y* - y) x

- **Erreur** : y* - y (cible moins prédiction)
- **Direction** : x (vecteur d'entrée)
- **Taux d'apprentissage** : η contrôle la vitesse de convergence

### Convergence
- **Théorème de Novikoff** : convergence en nombre fini d'itérations si données linéairement séparables
- **Borne** : nombre d'erreurs ≤ (R/γ)² où R est le rayon des données et γ la marge
- **Limites** : XOR non séparable linéairement (Minsky & Papert, 1969)

## Extensions modernes

### Perceptron multicouche (MLP)
- **Couches cachées** : représentations intermédiaires
- **Non-linéarité** : fonctions d'activation entre couches
- **Backpropagation** : apprentissage par rétropropagation du gradient

### Perceptron moyenné
- **Moyenne des poids** : réduction de la variance
- **Online learning** : apprentissage séquentiel
- **Robustesse** : moins sensible à l'ordre des exemples

## Applications

### Classifieur linéaire
- **SVM** : maximisation de la marge
- **Régression logistique** : probabilités de classification
- **Large margin** : meilleure généralisation

### Traitement du signal
- **Filtres adaptatifs** : annulation de bruit
- **Égalisation** : correction de canal de communication
- **Beamforming** : antennes intelligentes
