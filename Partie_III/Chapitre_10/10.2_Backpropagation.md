# 10.2. Backpropagation

## Problème de l'apprentissage profond

### Credit assignment problem
Comment attribuer la responsabilité de l'erreur aux neurones des couches internes ?

### Chaîne de dérivation
- **Composition** : f = f_L ∘ f_{L-1} ∘ ... ∘ f_1
- **Règle de chaîne** : ∂f/∂θ = ∂f/∂h_L · ∂h_L/∂h_{L-1} · ... · ∂h_1/∂θ

## Algorithme de backpropagation

### Forward pass (propagation avant)
- **Calcul des activations** : h_l = σ(W_l h_{l-1} + b_l)
- **Propagation** : couche par couche de l'entrée vers la sortie

### Backward pass (rétropropagation)
- **Erreur de sortie** : δ_L = ∂L/∂h_L · σ'(z_L)
- **Propagation arrière** : δ_l = W_{l+1}^T δ_{l+1} · σ'(z_l)
- **Gradients** : ∂L/∂W_l = δ_l h_{l-1}^T

### Mise à jour des paramètres
W_l ← W_l - η ∂L/∂W_l
b_l ← b_l - η ∂L/∂b_l

## Fonctions d'activation

### Sigmoïde
σ(x) = 1/(1 + e^{-x})

- **Dérivée** : σ'(x) = σ(x)(1 - σ(x))
- **Problème** : vanishing gradient pour |x| grand
- **Interprétation** : probabilité de sortie

### Tangente hyperbolique
tanh(x) = (e^x - e^{-x})/(e^x + e^{-x})

- **Dérivée** : 1 - tanh²(x)
- **Zéro-centré** : moyenne des sorties proche de 0
- **Meilleur** : que sigmoïde pour couches cachées

### ReLU (Rectified Linear Unit)
ReLU(x) = max(0, x)

- **Dérivée** : 1 si x > 0, 0 sinon
- **Avantages** : pas de saturation, calcul rapide
- **Problème** : dying ReLU (neurones morts si x < 0 toujours)

### Variantes de ReLU
- **Leaky ReLU** : max(αx, x) avec α petit (ex: 0.01)
- **ELU** : α(e^x - 1) si x < 0, x sinon
- **GELU** : x · Φ(x) utilisé dans Transformers

## Problèmes et solutions

### Vanishing gradient
- **Cause** : dérivées < 1 s'accumulent multiplicativement
- **Solutions** : ReLU, connexions résiduelles, batch normalization
- **Impact** : couches profondes n'apprennent pas

### Exploding gradient
- **Cause** : dérivées > 1 s'accumulent
- **Solutions** : gradient clipping, initialisation soignée
- **Impact** : instabilité numérique, NaN

## Optimisations computationnelles

### Vectorisation
- **Matrices** : traitement par mini-batch
- **GPU** : calcul parallèle matriciel massif
- **Autograd** : différentiation automatique (PyTorch, TensorFlow)

### Implémentations efficaces
- **Graph computation** : graphe de calcul optimisé
- **Just-in-time compilation** : XLA, TorchScript
- **Distributed training** : plusieurs GPU/TPU
