# 10.4. Géométrie de l'espace latent

## Espace latent : définition

### Concept fondamental
Espace de dimension réduite capturant l'essence des données

### Autoencodeurs
- **Encoder** : x → z (compression vers l'espace latent)
- **Decoder** : z → x̂ (reconstruction depuis l'espace latent)
- **Bottleneck** : dimension latente << dimension d'entrée

### Variational Autoencoders (VAE)
- **Distribution latente** : q(z|x) = N(μ(x), σ(x))
- **ELBO** : reconstruction + régularisation KL
- **Génération** : échantillonnage z ~ N(0,I) puis décodage

## Géométrie des représentations

### Manifolds (variétés)
- **Hypothèse du manifold** : données sur variété de basse dimension
- **Apprentissage** : découverte de la géométrie intrinsèque
- **Généralisation** : interpolation le long de la variété

### Distances et métriques
- **Euclidienne** : distance L2 standard
- **Cosine** : similarité directionnelle (angle)
- **Mahalanobis** : métrique adaptée à la covariance

### Clustering dans l'espace latent
- **K-means** : centroïdes dans l'espace latent
- **GMM** : mélanges gaussiens
- **DBSCAN** : clustering par densité

## Applications géométriques

### Visualisation : t-SNE et UMAP
- **t-SNE** : préservation des voisinages locaux
- **UMAP** : préservation structure globale et locale
- **Non-linéaire** : capture variétés complexes

### Contrastive learning
- **SimCLR** : apprentissage par contraste positif/négatif
- **NT-Xent loss** : attraction des positifs, répulsion des négatifs
- **Géométrie** : séparation des classes dans l'espace latent

### Normalizing flows
- **Transformation bijective** : z = f(x) avec jacobien calculable
- **Densité exacte** : p_x = p_z / |det df/dx|
- **Génération** : échantillonnage exact de la distribution

## Propriétés émergentes

### Disentanglement
- **Facteurs indépendants** : traits séparés dans l'espace latent
- **β-VAE** : régularisation pour encourager la séparation
- **Interprétabilité** : contrôle individuel des facteurs

### Représentations hiérarchiques
- **Pyramidal** : du grossier au fin
- **Multi-scale** : différentes résolutions
- **Composition** : parties élémentaires combinées

### Invariances apprises
- **Translation** : via convolutions
- **Rotation** : via data augmentation
- **Échelle** : via multi-scale processing
