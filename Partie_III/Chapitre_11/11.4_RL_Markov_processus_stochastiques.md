# 11.4. RL, Markov et processus stochastiques

## Chaînes de Markov

### Définition
P(X_{t+1} = j | X_t = i, X_{t-1}, ...) = P(X_{t+1} = j | X_t = i)

- **Propriété de Markov** : futur indépendant du passé donné le présent
- **Matrice de transition** : P_ij = probabilité de passer de i à j

### Classification des états
- **Irréductible** : une seule classe communicante
- **Récurrent** : retour certain à l'état
- **Transient** : probabilité de retour < 1
- **Périodique** : retour par multiples d'une période k

### Distribution stationnaire
π P = π

- **Existence** : pour chaînes irréductibles, apériodiques
- **Unicité** : convergence depuis toute distribution initiale
- **Interprétation** : proportion de temps passé dans chaque état

## Processus de décision markoviens (MDP)

### Composants d'un MDP
- **États** : S ensemble des états possibles
- **Actions** : A(s) actions disponibles dans l'état s
- **Récompenses** : R(s,a,s') récompense immédiate
- **Transitions** : P(s'|s,a) probabilité de transition

### Équation de Bellman (valeur optimale)
V*(s) = max_a [R(s,a) + γ Σ_{s'} P(s'|s,a) V*(s')]

- **γ** : facteur de discount (0 < γ < 1)
- **Politique optimale** : π*(s) = argmax_a Q*(s,a)
- **Q-function** : Q*(s,a) = R(s,a) + γ Σ P(s'|s,a) V*(s')

## Apprentissage par renforcement

### Q-learning (off-policy)
Q(s,a) ← Q(s,a) + α [r + γ max_a' Q(s',a') - Q(s,a)]

- **Off-policy** : apprend la politique optimale indépendamment de l'exploration
- **Exploration** : ε-greedy, softmax, UCB
- **Convergence** : garantie sous conditions (Watkins, 1989)

### SARSA (on-policy)
Q(s,a) ← Q(s,a) + α [r + γ Q(s',a') - Q(s,a)]

- **On-policy** : suit la politique d'exploration
- **SARSA** : State-Action-Reward-State-Action
- **Plus conservateur** : que Q-learning

### Deep Reinforcement Learning
- **DQN** : Q-network avec réseau profond
- **Experience replay** : mémoire tampon pour stabilité
- **Target network** : réseau cible mis à jour périodiquement

## Applications avancées

### Multi-agent RL
- **Jeux non-coopératifs** : équilibre de Nash
- **Coordination** : communication implicite entre agents
- **Apprentissage** : par interaction mutuelle

### Inverse RL
- **Apprentissage de récompenses** : à partir de démonstrations expertes
- **IRL** : Inverse Reinforcement Learning
- **Imitation learning** : apprentissage par imitation

### Hierarchical RL
- **Options** : actions temporelles étendues (macro-actions)
- **Feudal RL** : architecture manager-worker
- **Skill discovery** : apprentissage automatique de compétences
