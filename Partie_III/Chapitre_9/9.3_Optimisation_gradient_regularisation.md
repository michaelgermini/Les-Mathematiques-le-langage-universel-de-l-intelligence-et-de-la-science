# 9.3. Optimisation : gradient, descentes, régularisation

## Introduction

L'entraînement d'un modèle d'apprentissage automatique est un problème d'**optimisation** : trouver les paramètres θ qui minimisent une fonction de perte L(θ). Ce chapitre présente les algorithmes fondamentaux et les techniques de régularisation qui permettent d'obtenir des modèles performants et généralisables.

---

## 1. Descente de gradient

### 1.1 Gradient Descent (GD)

#### Principe
Itérer dans la direction opposée au gradient :
```
θₜ₊₁ = θₜ - η ∇L(θₜ)
```
où η > 0 est le **taux d'apprentissage** (learning rate).

#### Intuition géométrique
- Le gradient ∇L pointe vers la direction de plus forte augmentation
- On se déplace dans la direction opposée pour diminuer L

#### Convergence
Pour une fonction L-lisse et μ-fortement convexe :
```
L(θₜ) - L(θ*) ≤ (1 - μ/L)ᵗ [L(θ₀) - L(θ*)]
```
Convergence linéaire avec taux (1 - μ/L).

#### Limitations
- Calcule le gradient sur **toutes** les données → coûteux
- Peut osciller dans les vallées étroites
- Sensible au choix de η

### 1.2 Stochastic Gradient Descent (SGD)

#### Principe
Utiliser un **estimateur non biaisé** du gradient :
```
θₜ₊₁ = θₜ - η ∇L(θₜ; ξₜ)
```
où ξₜ est un échantillon (ou mini-batch) aléatoire.

#### Propriétés
- **Espérance** : E[∇L(θ; ξ)] = ∇L(θ)
- **Variance** : introduit du bruit, mais permet d'échapper aux minima locaux
- **Coût** : O(batch_size) vs O(n) pour GD

#### Mini-batch SGD
```
∇L(θ; B) = (1/|B|) Σᵢ∈B ∇Lᵢ(θ)
```
Compromis entre variance (petit batch) et efficacité (grand batch).

| Taille batch | Variance | Parallélisme | Généralisation |
|--------------|----------|--------------|----------------|
| 1 | Haute | Faible | Bonne |
| 32-256 | Moyenne | Bon | Bonne |
| 1000+ | Faible | Excellent | Peut dégrader |

### 1.3 Momentum

#### Principe
Accumuler une "vitesse" dans les directions persistantes :
```
vₜ = γ vₜ₋₁ + η ∇L(θₜ)
θₜ₊₁ = θₜ - vₜ
```
où γ ∈ [0,1) est le coefficient de momentum (typiquement 0.9).

#### Avantages
- **Accélération** dans les directions cohérentes
- **Amortissement** des oscillations
- Aide à traverser les plateaux

#### Nesterov Accelerated Gradient (NAG)
Évalue le gradient au point "anticipé" :
```
vₜ = γ vₜ₋₁ + η ∇L(θₜ - γ vₜ₋₁)
θₜ₊₁ = θₜ - vₜ
```
Convergence plus rapide (O(1/t²) vs O(1/t) pour convexe).

---

## 2. Optimiseurs adaptatifs

### 2.1 AdaGrad

#### Principe
Adapter le learning rate par paramètre selon l'historique des gradients :
```
Gₜ = Gₜ₋₁ + (∇L(θₜ))²    (élément par élément)
θₜ₊₁ = θₜ - η/√(Gₜ + ε) ⊙ ∇L(θₜ)
```

#### Propriétés
- Paramètres fréquemment mis à jour → learning rate diminue
- Bon pour les données sparse (NLP, recommandation)
- **Problème** : Gₜ croît monotonement → learning rate → 0

### 2.2 RMSProp

#### Principe
Utiliser une moyenne mobile exponentielle au lieu de la somme :
```
E[g²]ₜ = γ E[g²]ₜ₋₁ + (1-γ) (∇L(θₜ))²
θₜ₊₁ = θₜ - η/√(E[g²]ₜ + ε) ⊙ ∇L(θₜ)
```
où γ ≈ 0.9.

#### Avantages
- Pas de décroissance monotone du learning rate
- Adapté aux problèmes non-stationnaires
- Efficace pour les RNN

### 2.3 Adam (Adaptive Moment Estimation)

#### Principe
Combine momentum et adaptation :
```
mₜ = β₁ mₜ₋₁ + (1-β₁) ∇L(θₜ)         (1er moment)
vₜ = β₂ vₜ₋₁ + (1-β₂) (∇L(θₜ))²      (2ème moment)
m̂ₜ = mₜ / (1 - β₁ᵗ)                   (correction de biais)
v̂ₜ = vₜ / (1 - β₂ᵗ)
θₜ₊₁ = θₜ - η m̂ₜ / (√v̂ₜ + ε)
```

#### Hyperparamètres typiques
- β₁ = 0.9 (momentum)
- β₂ = 0.999 (adaptation)
- ε = 10⁻⁸ (stabilité numérique)
- η = 0.001 (learning rate)

#### Variantes
| Variante | Modification |
|----------|--------------|
| **AdamW** | Weight decay découplé |
| **RAdam** | Warm-up automatique |
| **NAdam** | Nesterov momentum |
| **AdaFactor** | Mémoire réduite |
| **LAMB** | Pour très grands batchs |

### 2.4 Comparaison des optimiseurs

| Optimiseur | Avantages | Inconvénients |
|------------|-----------|---------------|
| **SGD + Momentum** | Généralise bien | Sensible à η |
| **Adam** | Converge vite, peu de tuning | Peut mal généraliser |
| **AdamW** | Meilleur que Adam | - |
| **SGD avec schedule** | Meilleure généralisation | Plus de tuning |

---

## 3. Scheduling du learning rate

### 3.1 Décroissance

#### Step decay
```
η(t) = η₀ × γ^⌊t/s⌋
```
Réduit η par facteur γ tous les s steps.

#### Exponential decay
```
η(t) = η₀ × e^{-λt}
```

#### Cosine annealing
```
η(t) = η_min + ½(η_max - η_min)(1 + cos(πt/T))
```
Décroissance douce avec possibilité de restarts.

### 3.2 Warm-up

Augmenter progressivement η au début :
```
η(t) = η_max × t/T_warmup    pour t < T_warmup
```

**Motivation** : stabilise l'entraînement initial, surtout avec grands batchs.

### 3.3 Cyclical learning rates

Osciller entre η_min et η_max :
- Aide à échapper aux minima locaux
- Explore différentes régions de l'espace des paramètres

---

## 4. Régularisation

### 4.1 Régularisation L2 (Ridge / Weight Decay)

#### Formulation
```
L_reg(θ) = L(θ) + λ/2 ||θ||²
```

#### Effet
- **Shrinkage** : pousse les poids vers zéro
- **Interprétation bayésienne** : prior Gaussien N(0, 1/λ)
- **Solution analytique** (régression) : θ̂ = (X^T X + λI)⁻¹ X^T y

#### Weight decay vs L2
Avec Adam, weight decay découplé est préférable :
```
θₜ₊₁ = θₜ - η(m̂ₜ/√v̂ₜ + λθₜ)
```

### 4.2 Régularisation L1 (Lasso)

#### Formulation
```
L_reg(θ) = L(θ) + λ ||θ||₁
```

#### Effet
- **Sparsité** : certains poids deviennent exactement 0
- **Sélection de features** : élimine les variables non pertinentes
- **Interprétation bayésienne** : prior Laplacien

#### Optimisation
Non différentiable en 0 → méthodes proximales :
```
θₜ₊₁ = prox_{λη}(θₜ - η∇L(θₜ))
```
où prox est le soft-thresholding.

### 4.3 Elastic Net

#### Formulation
```
L_reg(θ) = L(θ) + λ₁||θ||₁ + λ₂||θ||²
```

#### Avantages
- Combine sparsité (L1) et stabilité (L2)
- Gère les groupes de variables corrélées
- Plus robuste que Lasso seul

### 4.4 Dropout

#### Principe
Pendant l'entraînement, désactiver aléatoirement des neurones :
```
h̃ = h ⊙ m, où m ~ Bernoulli(1-p)
```
p est le taux de dropout (typiquement 0.1-0.5).

#### Interprétation
- **Ensemble** : moyenne implicite de 2ⁿ sous-réseaux
- **Co-adaptation** : empêche les neurones de trop dépendre les uns des autres
- **Régularisation** : équivalent approximatif à L2

#### À l'inférence
Désactiver le dropout et multiplier par (1-p) :
```
h_test = (1-p) × h
```
ou utiliser "inverted dropout" pendant l'entraînement.

### 4.5 Autres techniques

| Technique | Principe |
|-----------|----------|
| **Early stopping** | Arrêter quand validation loss augmente |
| **Data augmentation** | Augmenter artificiellement les données |
| **Label smoothing** | Adoucir les one-hot targets |
| **Mixup** | Interpoler des paires d'exemples |
| **Batch normalization** | Normaliser les activations (effet régularisant) |

---

## 5. Méthodes avancées

### 5.1 Méthodes du second ordre

#### Newton
```
θₜ₊₁ = θₜ - H⁻¹ ∇L(θₜ)
```
où H = ∇²L est la Hessienne.

**Avantage** : convergence quadratique près de l'optimum.
**Problème** : H est n×n, coûteux à calculer et inverser.

#### Quasi-Newton (BFGS, L-BFGS)
Approxime H⁻¹ itérativement sans la calculer explicitement.
- **L-BFGS** : version à mémoire limitée
- Utilisé pour l'optimisation convexe de taille moyenne

#### Natural gradient
```
θₜ₊₁ = θₜ - η F⁻¹ ∇L(θₜ)
```
où F est la matrice d'information de Fisher.
- Invariant aux reparamétrisations
- Approximations : K-FAC, EKFAC

### 5.2 Optimisation contrainte

#### Lagrangien
Pour min L(θ) sous g(θ) ≤ 0 :
```
L(θ, λ) = L(θ) + λ g(θ)
```

#### Conditions KKT
- Stationnarité : ∇L + λ∇g = 0
- Faisabilité primale : g(θ) ≤ 0
- Faisabilité duale : λ ≥ 0
- Complémentarité : λ g(θ) = 0

#### Méthodes
- **Projected gradient** : projeter sur l'ensemble faisable
- **Barrier methods** : ajouter une barrière log
- **ADMM** : décomposition pour problèmes séparables

### 5.3 Optimisation non-convexe

#### Défis
- Minima locaux
- Points selles (saddle points)
- Plateaux

#### Stratégies
- **Restarts** : plusieurs initialisations
- **Noise injection** : SGD aide naturellement
- **Learning rate schedules** : exploration puis exploitation
- **Sharpness-aware minimization (SAM)** : cherche des minima "plats"

---

## 6. Considérations pratiques

### 6.1 Choix du learning rate

#### Règles empiriques
- Commencer avec η = 0.001 pour Adam
- Commencer avec η = 0.1 pour SGD
- Réduire si instable, augmenter si trop lent

#### Learning rate finder
1. Augmenter η exponentiellement pendant quelques epochs
2. Tracer loss vs η
3. Choisir η juste avant que la loss explose

### 6.2 Taille de batch

| Petit batch | Grand batch |
|-------------|-------------|
| Plus de bruit (régularisation implicite) | Moins de bruit |
| Meilleure généralisation | Convergence plus rapide |
| Moins parallélisable | Très parallélisable |
| η plus petit | η plus grand (scaling linéaire) |

### 6.3 Gradient clipping

Éviter l'explosion des gradients :
```
g ← g × min(1, τ/||g||)
```
Essentiel pour les RNN et Transformers.

---

## Conclusion

L'optimisation en deep learning est un art autant qu'une science :

1. **SGD + Momentum** reste compétitif avec un bon schedule
2. **Adam/AdamW** convergent vite avec peu de tuning
3. **La régularisation** (L2, dropout, early stopping) est essentielle
4. **Le learning rate** est l'hyperparamètre le plus important

La compréhension des propriétés mathématiques (convexité, conditionnement, paysage de perte) guide les choix pratiques et le diagnostic des problèmes.
