# 9.1. Modèles statistiques pour l'apprentissage

## Introduction

L'apprentissage automatique repose sur des modèles statistiques qui formalisent la relation entre les données observées et les quantités à prédire. Ce chapitre présente les fondements mathématiques de ces modèles, de la régression linéaire aux modèles à variables latentes.

---

## 1. Apprentissage supervisé

L'apprentissage supervisé consiste à apprendre une fonction f : X → Y à partir de paires (xᵢ, yᵢ).

### 1.1 Régression linéaire

#### Modèle
```
y = w·x + b + ε, où ε ~ N(0, σ²)
```

#### Estimation par moindres carrés
Minimise la somme des carrés des résidus :
```
ŵ = argmin_w Σᵢ (yᵢ - w·xᵢ - b)²
```

**Solution analytique** (équations normales) :
```
ŵ = (X^T X)^{-1} X^T y
```

#### Propriétés BLUE
Sous les hypothèses de Gauss-Markov :
- **Best** : variance minimale parmi les estimateurs linéaires
- **Linear** : combinaison linéaire des observations
- **Unbiased** : E[ŵ] = w (sans biais)
- **Estimator** : fonction des données

#### Interprétation géométrique
- ŷ est la projection orthogonale de y sur l'espace engendré par les colonnes de X
- Les résidus sont orthogonaux à cet espace

### 1.2 Régression logistique

#### Modèle
Pour la classification binaire :
```
P(y=1|x) = σ(w·x + b) = 1/(1 + e^{-(w·x+b)})
```
où σ est la fonction sigmoïde.

#### Log-odds (logit)
```
log(P(y=1)/P(y=0)) = w·x + b
```
Le modèle est linéaire dans l'espace des log-odds.

#### Estimation par maximum de vraisemblance
```
L(w) = Πᵢ P(yᵢ|xᵢ)^{yᵢ} (1-P(yᵢ|xᵢ))^{1-yᵢ}
```

**Log-vraisemblance** :
```
ℓ(w) = Σᵢ [yᵢ log(σ(w·xᵢ)) + (1-yᵢ) log(1-σ(w·xᵢ))]
```

Pas de solution analytique → optimisation itérative (Newton-Raphson, gradient).

#### Extension multiclasse : Softmax
```
P(y=k|x) = exp(wₖ·x) / Σⱼ exp(wⱼ·x)
```

### 1.3 Modèles linéaires généralisés (GLM)

#### Structure générale
Un GLM est défini par :
1. **Distribution** : Y suit une loi de la famille exponentielle
2. **Prédicteur linéaire** : η = X·β
3. **Fonction de lien** : g(μ) = η, où μ = E[Y]

#### Exemples

| Distribution | Lien canonique | Application |
|--------------|----------------|-------------|
| Normale | Identité | Régression linéaire |
| Bernoulli | Logit | Classification binaire |
| Poisson | Log | Comptage |
| Gamma | Inverse | Temps, coûts |
| Binomiale négative | Log | Comptage surdispersé |

#### Estimation : IRLS
**Iteratively Reweighted Least Squares** :
1. Initialiser β
2. Calculer η = Xβ, puis μ = g⁻¹(η)
3. Calculer les poids W et la variable de travail z
4. Résoudre : β ← (X^T W X)^{-1} X^T W z
5. Répéter jusqu'à convergence

---

## 2. Apprentissage non supervisé

Découvrir des structures cachées sans étiquettes.

### 2.1 Mélange de Gaussiennes (GMM)

#### Modèle
La densité est un mélange de K Gaussiennes :
```
p(x) = Σₖ πₖ N(x | μₖ, Σₖ)
```
où :
- πₖ : poids du composant k (Σₖ πₖ = 1)
- μₖ : moyenne du composant k
- Σₖ : matrice de covariance du composant k

#### Variable latente
z ∈ {1, ..., K} indique le composant :
```
P(z=k) = πₖ
p(x|z=k) = N(x | μₖ, Σₖ)
```

#### Algorithme EM (Expectation-Maximization)

**E-step** : calculer les responsabilités
```
γₖ(xᵢ) = P(zᵢ=k|xᵢ) = πₖ N(xᵢ|μₖ,Σₖ) / Σⱼ πⱼ N(xᵢ|μⱼ,Σⱼ)
```

**M-step** : mettre à jour les paramètres
```
Nₖ = Σᵢ γₖ(xᵢ)
μₖ = (1/Nₖ) Σᵢ γₖ(xᵢ) xᵢ
Σₖ = (1/Nₖ) Σᵢ γₖ(xᵢ) (xᵢ-μₖ)(xᵢ-μₖ)^T
πₖ = Nₖ/N
```

#### Propriétés
- Converge vers un maximum local de la vraisemblance
- Sensible à l'initialisation
- Choix de K : BIC, validation croisée

### 2.2 Analyse en composantes principales (PCA)

#### Objectif
Trouver les directions de variance maximale.

#### Formulation
Maximiser :
```
max_w ||Xw||² sous ||w||=1
```

**Solution** : w est le vecteur propre de X^T X associé à la plus grande valeur propre.

#### Décomposition
```
X ≈ U_k Σ_k V_k^T
```
où k est le nombre de composantes retenues.

#### Applications
- Réduction de dimensionnalité
- Visualisation
- Prétraitement (blanchiment)
- Compression

### 2.3 Analyse factorielle

#### Modèle
```
x = Λf + ε
```
où :
- f ∈ ℝᵏ : facteurs latents (k << p)
- Λ ∈ ℝᵖˣᵏ : matrice de charges
- ε ~ N(0, Ψ) : bruit diagonal

#### Différence avec PCA
- PCA : décomposition déterministe
- Analyse factorielle : modèle probabiliste avec bruit

#### Rotation
Les facteurs ne sont identifiables qu'à une rotation près.
- **Varimax** : maximise la variance des charges (interprétabilité)
- **Promax** : rotation oblique (facteurs corrélés)

### 2.4 Modèles de Markov cachés (HMM)

#### Structure
- **États cachés** : z₁, z₂, ..., zₜ (chaîne de Markov)
- **Observations** : x₁, x₂, ..., xₜ (conditionnelles aux états)

#### Paramètres
- **Transition** : A[i,j] = P(zₜ₊₁=j | zₜ=i)
- **Émission** : B[j,k] = P(xₜ=k | zₜ=j) ou p(xₜ|zₜ)
- **Initial** : π[i] = P(z₁=i)

#### Algorithmes fondamentaux

| Problème | Algorithme | Complexité |
|----------|------------|------------|
| Vraisemblance P(x) | Forward | O(TK²) |
| Inférence P(zₜ|x) | Forward-Backward | O(TK²) |
| Décodage argmax_z P(z|x) | Viterbi | O(TK²) |
| Apprentissage | Baum-Welch (EM) | O(TK²) par itération |

#### Applications
- Reconnaissance vocale
- Annotation de séquences biologiques
- Finance (régimes de marché)

---

## 3. Validation et généralisation

### 3.1 Décomposition biais-variance

L'erreur de généralisation se décompose :
```
E[(ŷ - y*)²] = Biais² + Variance + Bruit
```

| Terme | Définition | Cause |
|-------|------------|-------|
| **Biais** | (E[ŷ] - y*)² | Modèle trop simple |
| **Variance** | E[(ŷ - E[ŷ])²] | Modèle trop flexible |
| **Bruit** | σ² | Irréductible |

### 3.2 Validation croisée

#### K-fold CV
1. Diviser les données en K plis
2. Pour k = 1 à K :
   - Entraîner sur K-1 plis
   - Évaluer sur le pli k
3. Moyenner les erreurs

**Choix de K** :
- K=5 ou K=10 : bon compromis
- K=N (Leave-One-Out) : variance élevée, biais faible

#### Validation croisée imbriquée (Nested CV)
- Boucle externe : estimation de l'erreur de généralisation
- Boucle interne : sélection des hyperparamètres

### 3.3 Sélection de modèles

#### Critères d'information

| Critère | Formule | Pénalité |
|---------|---------|----------|
| **AIC** | -2ℓ + 2k | Légère |
| **BIC** | -2ℓ + k log(n) | Plus forte |
| **MDL** | Longueur de description minimale | Théorie de l'information |

où ℓ est la log-vraisemblance et k le nombre de paramètres.

#### Régularisation
- **L2 (Ridge)** : ∥w∥² — shrinkage vers zéro
- **L1 (Lasso)** : ∥w∥₁ — sélection de variables
- **Elastic Net** : combinaison L1 + L2

---

## 4. Modèles bayésiens

### 4.1 Approche bayésienne

Au lieu d'estimer un point w*, on calcule une distribution :
```
p(w|D) = p(D|w) p(w) / p(D)
```

- **Prior** p(w) : croyances a priori
- **Vraisemblance** p(D|w) : adéquation aux données
- **Posterior** p(w|D) : croyances mises à jour

### 4.2 Régression bayésienne

#### Prior conjugué
Pour une régression linéaire avec prior Gaussien :
```
w ~ N(0, τ²I)
```

Le posterior est aussi Gaussien :
```
p(w|X,y) = N(w | μ_post, Σ_post)
```
avec formules explicites.

#### Prédiction
```
p(y*|x*,D) = ∫ p(y*|x*,w) p(w|D) dw
```
Intègre l'incertitude sur les paramètres.

### 4.3 Sélection de modèles bayésienne

**Evidence** (vraisemblance marginale) :
```
p(D|M) = ∫ p(D|w,M) p(w|M) dw
```

Pénalise automatiquement la complexité (rasoir d'Occam bayésien).

---

## Conclusion

Les modèles statistiques fournissent le cadre mathématique rigoureux pour :

1. **Formaliser** les hypothèses sur les données (distributions, structures)
2. **Estimer** les paramètres de manière optimale (MLE, MAP, bayésien)
3. **Quantifier l'incertitude** (intervalles de confiance, distributions posterieures)
4. **Sélectionner** le bon niveau de complexité (biais-variance, critères d'information)

Ces fondements théoriques sont essentiels pour comprendre les performances et les limites des algorithmes d'apprentissage automatique.
