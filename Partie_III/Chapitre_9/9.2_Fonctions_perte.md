# 9.2. Fonctions de perte

## Introduction

La **fonction de perte** (ou fonction de coût) quantifie l'écart entre les prédictions d'un modèle et les valeurs réelles. Le choix de cette fonction est crucial : il détermine ce que le modèle optimise et influence ses propriétés (robustesse, interprétabilité, comportement aux extrêmes).

---

## 1. Pertes pour la régression

### 1.1 Erreur quadratique (MSE)

#### Définition
```
L(y, ŷ) = (y - ŷ)²
```

**Perte moyenne** (Mean Squared Error) :
```
MSE = (1/n) Σᵢ (yᵢ - ŷᵢ)²
```

#### Propriétés
| Propriété | Valeur |
|-----------|--------|
| **Convexité** | ✓ Strictement convexe |
| **Différentiabilité** | ✓ Partout |
| **Gradient** | ∂L/∂ŷ = -2(y - ŷ) |
| **Robustesse** | ✗ Sensible aux outliers |

#### Interprétation statistique
- Minimiser MSE équivaut à maximiser la vraisemblance sous hypothèse de bruit gaussien
- L'estimateur optimal est la **moyenne conditionnelle** E[Y|X]

#### Variantes
- **RMSE** : √MSE (même unité que y)
- **Normalized MSE** : MSE / Var(y)
- **R²** : 1 - MSE/Var(y) (coefficient de détermination)

### 1.2 Erreur absolue (MAE)

#### Définition
```
L(y, ŷ) = |y - ŷ|
```

**Perte moyenne** (Mean Absolute Error) :
```
MAE = (1/n) Σᵢ |yᵢ - ŷᵢ|
```

#### Propriétés
| Propriété | Valeur |
|-----------|--------|
| **Convexité** | ✓ Convexe |
| **Différentiabilité** | ✗ Non différentiable en 0 |
| **Sous-gradient** | ∂L/∂ŷ ∈ {-1} si y > ŷ, {+1} si y < ŷ |
| **Robustesse** | ✓ Plus robuste que MSE |

#### Interprétation statistique
- L'estimateur optimal est la **médiane conditionnelle**
- Moins sensible aux valeurs extrêmes

### 1.3 Perte de Huber

#### Définition
```
L_δ(y, ŷ) = 
  ½(y - ŷ)²           si |y - ŷ| ≤ δ
  δ|y - ŷ| - ½δ²      sinon
```

#### Propriétés
- **Hybride** : quadratique près de 0, linéaire loin
- **Différentiable** : partout (contrairement à MAE)
- **Paramètre δ** : contrôle la transition

#### Avantages
- Robuste aux outliers (comme MAE)
- Gradient bien défini (comme MSE)
- Convergence plus stable

### 1.4 Pertes quantiles

#### Définition (Quantile Loss)
Pour le quantile τ ∈ (0,1) :
```
L_τ(y, ŷ) = 
  τ(y - ŷ)       si y ≥ ŷ
  (1-τ)(ŷ - y)   sinon
```

#### Applications
- **τ = 0.5** : médiane (équivalent à MAE)
- **τ = 0.1, 0.9** : intervalles de prédiction
- **Régression quantile** : prédire plusieurs quantiles

### 1.5 Log-cosh

#### Définition
```
L(y, ŷ) = log(cosh(y - ŷ))
```

#### Propriétés
- Approxime MSE pour petites erreurs
- Approxime MAE pour grandes erreurs
- Deux fois différentiable partout

---

## 2. Pertes pour la classification

### 2.1 Perte 0-1

#### Définition
```
L(y, ŷ) = 𝟙[y ≠ ŷ] = 
  0 si y = ŷ
  1 sinon
```

#### Propriétés
- **Intuitive** : compte les erreurs
- **Non-convexe** : optimisation NP-difficile
- **Non-différentiable** : pas de gradient

En pratique, on utilise des **surrogates convexes**.

### 2.2 Entropie croisée (Cross-Entropy)

#### Classification binaire
```
L(y, p) = -[y log(p) + (1-y) log(1-p)]
```
où p = P(ŷ=1).

#### Classification multiclasse
```
L(y, p) = -Σₖ yₖ log(pₖ)
```
où y est one-hot et p = softmax(logits).

#### Propriétés
| Propriété | Valeur |
|-----------|--------|
| **Convexité** | ✓ (en logits) |
| **Différentiabilité** | ✓ |
| **Gradient** | ∂L/∂logits = p - y |
| **Interprétation** | Maximum de vraisemblance |

#### Relation avec KL-divergence
```
H(y, p) = H(y) + D_KL(y || p)
```
Minimiser l'entropie croisée = minimiser la divergence KL.

### 2.3 Perte Hinge (SVM)

#### Définition
```
L(y, f) = max(0, 1 - y·f)
```
où y ∈ {-1, +1} et f est le score (non probabilité).

#### Propriétés
- **Marge** : pénalise si y·f < 1 (même si correct)
- **Sparsité** : seuls les support vectors contribuent
- **Non-différentiable** : en y·f = 1

#### Variantes
- **Squared hinge** : max(0, 1 - y·f)²
- **Hinge multiclasse** : max(0, 1 + max_{k≠y} fₖ - f_y)

### 2.4 Perte exponentielle (AdaBoost)

#### Définition
```
L(y, f) = exp(-y·f)
```

#### Propriétés
- Pénalise fortement les erreurs avec grande marge négative
- Très sensible aux outliers
- Fondement théorique d'AdaBoost

### 2.5 Focal Loss

#### Définition
```
L(y, p) = -(1-p_t)^γ log(p_t)
```
où p_t = p si y=1, (1-p) sinon.

#### Motivation
- Réduit le poids des exemples faciles (p_t élevé)
- Focus sur les exemples difficiles
- **γ = 0** : entropie croisée standard
- **γ = 2** : valeur typique

#### Application
Détection d'objets avec déséquilibre de classes (beaucoup de background).

---

## 3. Pertes structurées

### 3.1 Pertes pour le ranking

#### Pairwise ranking loss
```
L = Σ_{(i,j): yᵢ > yⱼ} max(0, 1 - (fᵢ - fⱼ))
```
Pénalise les paires mal ordonnées.

#### Listwise losses
- **ListNet** : entropie croisée sur les permutations
- **ListMLE** : vraisemblance de la permutation
- **LambdaRank** : gradient pondéré par NDCG

### 3.2 Pertes pour la segmentation

#### Dice Loss
```
L = 1 - 2|A ∩ B| / (|A| + |B|)
```
Mesure le chevauchement entre prédiction et vérité.

#### Focal Tversky Loss
Généralisation pour gérer le déséquilibre de classes en segmentation.

### 3.3 Pertes pour les séquences

#### CTC Loss (Connectionist Temporal Classification)
- Marginalise sur tous les alignements possibles
- Utilisé en reconnaissance vocale, OCR

#### Sequence-to-sequence
- Teacher forcing : entropie croisée token par token
- Scheduled sampling : mélange progressif

---

## 4. Propriétés théoriques

### 4.1 Convexité

#### Importance
- **Convexe** : tout minimum local est global
- **Strictement convexe** : minimum unique
- **Non-convexe** : minima locaux, optimisation difficile

#### Classification des pertes

| Perte | Convexité |
|-------|-----------|
| MSE | Strictement convexe |
| MAE | Convexe |
| Huber | Strictement convexe |
| Cross-entropy | Convexe (en logits) |
| Hinge | Convexe |
| 0-1 | Non-convexe |

### 4.2 Calibration

Une perte est **propre** (proper) si le minimiseur est la vraie probabilité.

#### Pertes propres
- Entropie croisée (log loss)
- Brier score : (y - p)²

#### Pertes impropres
- Hinge loss (donne des scores, pas des probabilités)
- Nécessite une calibration post-hoc (Platt scaling, isotonic regression)

### 4.3 Robustesse

#### Fonction d'influence
Mesure la sensibilité à un point aberrant :
```
IF(x) = ∂ŵ/∂ε |_{ε=0}
```
où ε est le poids d'un point contaminant.

#### Classification
- **Bornée** : MAE, Huber (robustes)
- **Non bornée** : MSE, cross-entropy (sensibles)

### 4.4 Consistance de Fisher

Une perte est **Fisher-consistante** si minimiser le risque empirique converge vers le classifieur de Bayes optimal.

- Cross-entropy : ✓
- Hinge : ✓
- Exponential : ✓ (mais sensible au bruit)

---

## 5. Choix pratique

### 5.1 Guide de sélection

| Tâche | Perte recommandée | Alternative |
|-------|-------------------|-------------|
| Régression standard | MSE | Huber si outliers |
| Régression robuste | Huber, MAE | Quantile loss |
| Classification binaire | Cross-entropy | Focal si déséquilibre |
| Classification multiclasse | Cross-entropy + softmax | Hinge multiclasse |
| Ranking | Pairwise hinge | ListMLE |
| Segmentation | Dice + Cross-entropy | Focal Tversky |

### 5.2 Combinaisons

Souvent, on combine plusieurs pertes :
```
L_total = α L_1 + β L_2 + ...
```

Exemples :
- **GAN** : adversarial + reconstruction
- **VAE** : reconstruction + KL
- **Segmentation** : cross-entropy + Dice

### 5.3 Pondération adaptative

- **Uncertainty weighting** : apprend les poids automatiquement
- **GradNorm** : équilibre les gradients
- **Dynamic Task Prioritization** : focus sur les tâches difficiles

---

## Conclusion

Le choix de la fonction de perte encode les objectifs et les hypothèses du modèle :

1. **MSE** pour la régression standard (hypothèse gaussienne)
2. **Cross-entropy** pour la classification (maximum de vraisemblance)
3. **Huber/MAE** pour la robustesse aux outliers
4. **Pertes structurées** pour les tâches complexes (ranking, segmentation)

Comprendre les propriétés mathématiques des pertes (convexité, calibration, robustesse) permet de faire des choix éclairés et de diagnostiquer les problèmes d'entraînement.
