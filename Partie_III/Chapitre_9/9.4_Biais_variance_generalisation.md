# 9.4. Biais-variance et généralisation

## Introduction

Le but ultime de l'apprentissage automatique n'est pas de bien prédire sur les données d'entraînement, mais de **généraliser** à de nouvelles données. Ce chapitre explore les fondements théoriques de la généralisation : la décomposition biais-variance, la théorie de Vapnik-Chervonenkis, et les mystères de la généralisation en deep learning.

---

## 1. Décomposition biais-variance

### 1.1 Erreur de généralisation

Pour un problème de régression avec y = f(x) + ε où ε ~ N(0, σ²) :

L'**erreur de généralisation** du modèle ŷ = f̂(x) se décompose :
```
E[(ŷ - y)²] = Biais² + Variance + Bruit
```

#### Définitions

| Terme | Formule | Interprétation |
|-------|---------|----------------|
| **Biais** | E[f̂(x)] - f(x) | Erreur systématique |
| **Variance** | E[(f̂(x) - E[f̂(x)])²] | Sensibilité aux données |
| **Bruit** | σ² | Irréductible |

### 1.2 Le compromis (tradeoff)

```
Complexité ↑  →  Biais ↓, Variance ↑
Complexité ↓  →  Biais ↑, Variance ↓
```

#### Sous-apprentissage (Underfitting)
- Modèle trop simple
- Biais élevé, variance faible
- Erreur d'entraînement ET de test élevées

#### Sur-apprentissage (Overfitting)
- Modèle trop complexe
- Biais faible, variance élevée
- Erreur d'entraînement faible, erreur de test élevée

#### Point optimal
Minimise l'erreur totale = Biais² + Variance

### 1.3 Illustration

| Modèle | Biais | Variance | Comportement |
|--------|-------|----------|--------------|
| Constante (moyenne) | Élevé | 0 | Sous-apprend |
| Régression linéaire | Moyen | Faible | Équilibré |
| Polynôme degré 20 | Faible | Élevé | Sur-apprend |
| k-NN, k grand | Élevé | Faible | Lisse |
| k-NN, k=1 | Faible | Élevé | Mémorise |

### 1.4 Décomposition pour la classification

Pour la perte 0-1, la décomposition est plus complexe :
- **Biais** : différence avec le classifieur de Bayes
- **Variance** : fluctuations de la frontière de décision
- **Bruit de Bayes** : erreur irréductible

---

## 2. Théorie de l'apprentissage statistique

### 2.1 Cadre PAC (Probably Approximately Correct)

#### Définition
Un concept est **PAC-learnable** si, pour tout ε, δ > 0, il existe un algorithme qui, avec probabilité ≥ 1-δ, retourne une hypothèse d'erreur ≤ ε.

#### Complexité d'échantillon
Nombre d'exemples nécessaires :
```
n = O((1/ε) log(1/δ))
```
pour des classes finies d'hypothèses.

### 2.2 Dimension de Vapnik-Chervonenkis (VC)

#### Définition
La **dimension VC** d'une classe H est le plus grand nombre de points que H peut **shatter** (séparer de toutes les manières possibles).

#### Exemples

| Classe H | dim VC |
|----------|--------|
| Demi-plans en ℝ² | 3 |
| Hyperplans en ℝᵈ | d+1 |
| Rectangles en ℝ² | 4 |
| Cercles en ℝ² | 3 |
| Réseaux de neurones | O(W log W) où W = nb poids |

#### Borne de généralisation
Avec probabilité ≥ 1-δ :
```
R(h) ≤ R̂(h) + O(√(d_VC log(n) / n) + √(log(1/δ) / n))
```
où R est le risque réel, R̂ le risque empirique, d_VC la dimension VC.

### 2.3 Complexité de Rademacher

#### Définition
```
R_n(H) = E_σ[sup_{h∈H} (1/n) Σᵢ σᵢ h(xᵢ)]
```
où σᵢ ∈ {-1, +1} sont des variables de Rademacher.

#### Interprétation
Mesure la capacité de H à s'ajuster au bruit aléatoire.

#### Borne
```
R(h) ≤ R̂(h) + 2 R_n(H) + O(√(log(1/δ)/n))
```

### 2.4 Bornes PAC-Bayes

Pour une distribution Q sur les hypothèses et un prior P :
```
E_Q[R(h)] ≤ E_Q[R̂(h)] + √(D_KL(Q||P) + log(n/δ)) / (2n)
```

Fondement théorique de la régularisation bayésienne.

---

## 3. Contrôle de la complexité

### 3.1 Régularisation explicite

#### L2 (Ridge)
```
L_reg = L + λ||w||²
```
- Réduit la norme des poids
- Équivalent à un prior Gaussien

#### L1 (Lasso)
```
L_reg = L + λ||w||₁
```
- Induit la sparsité
- Sélection de features

### 3.2 Early stopping

#### Principe
Arrêter l'entraînement quand l'erreur de validation commence à augmenter.

#### Implémentation
```python
patience = 10
best_val_loss = inf
wait = 0

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        wait = 0
    else:
        wait += 1
        if wait >= patience:
            break
```

#### Interprétation
- Limite implicitement le nombre d'itérations
- Équivalent approximatif à la régularisation L2

### 3.3 Dropout

#### Mécanisme
Pendant l'entraînement, masquer aléatoirement des neurones :
```
h̃ᵢ = hᵢ × mᵢ, où mᵢ ~ Bernoulli(1-p)
```

#### Effets
- **Régularisation** : empêche la co-adaptation
- **Ensemble implicite** : moyenne de 2ⁿ sous-réseaux
- **Approximation bayésienne** : incertitude sur les poids

### 3.4 Data augmentation

#### Principe
Augmenter artificiellement le dataset avec des transformations préservant le label.

#### Exemples (images)
- Rotations, translations, flips
- Changements de couleur, contraste
- Crop aléatoire
- Mixup : x̃ = λxᵢ + (1-λ)xⱼ, ỹ = λyᵢ + (1-λ)yⱼ
- CutOut : masquer des régions

#### Effet théorique
- Augmente effectivement n (taille du dataset)
- Encode des invariances a priori

---

## 4. Méthodes ensemblistes

### 4.1 Bagging (Bootstrap Aggregating)

#### Principe
1. Créer B échantillons bootstrap (avec remise)
2. Entraîner un modèle sur chaque échantillon
3. Moyenner les prédictions (régression) ou voter (classification)

#### Effet sur biais-variance
- **Biais** : inchangé (même modèle de base)
- **Variance** : réduite d'un facteur ~1/B

#### Random Forest
Bagging + sélection aléatoire de features à chaque split.

### 4.2 Boosting

#### Principe
Entraîner séquentiellement des modèles faibles, chacun corrigeant les erreurs du précédent.

#### AdaBoost
```
αₜ = ½ log((1-εₜ)/εₜ)
wᵢ ← wᵢ × exp(-αₜ yᵢ hₜ(xᵢ))
```
où εₜ est l'erreur pondérée du modèle t.

#### Gradient Boosting
```
Fₜ(x) = Fₜ₋₁(x) + η hₜ(x)
```
où hₜ est ajusté sur les résidus -∂L/∂F.

#### XGBoost, LightGBM, CatBoost
Implémentations optimisées avec régularisation.

### 4.3 Stacking

#### Principe
1. Entraîner plusieurs modèles de base
2. Utiliser leurs prédictions comme features pour un méta-modèle

#### Implémentation
- Utiliser les prédictions out-of-fold pour éviter le leakage
- Le méta-modèle apprend à combiner les forces de chaque modèle

---

## 5. Généralisation en deep learning

### 5.1 Le paradoxe de la surparamétrisation

#### Observation
Les réseaux de neurones modernes ont souvent :
- Plus de paramètres que d'exemples (W >> n)
- Capacité de mémoriser des labels aléatoires
- Pourtant, ils généralisent bien !

#### Contradiction apparente
La théorie classique (VC, Rademacher) prédit un overfitting massif.

### 5.2 Double descent

#### Phénomène
```
Erreur de test
    │
    │    ╱╲
    │   ╱  ╲         ╱
    │  ╱    ╲       ╱
    │ ╱      ╲_____╱
    │╱
    └────────────────────→ Complexité
         ↑           ↑
    Interpolation  Surparamétrisation
      threshold
```

#### Régimes
1. **Sous-paramétrisé** : biais-variance classique
2. **Seuil d'interpolation** : pic d'erreur
3. **Surparamétrisé** : erreur redescend !

### 5.3 Biais implicite de SGD

#### Observations
- SGD converge vers des solutions de faible norme
- Les minima "plats" généralisent mieux que les minima "pointus"
- Le bruit de SGD agit comme un régulariseur

#### Sharpness et généralisation
- **Sharpness** : courbure locale de la perte
- Minima plats → plus robustes aux perturbations → meilleure généralisation

### 5.4 Neural Tangent Kernel (NTK)

#### Théorie
Dans la limite de largeur infinie, un réseau de neurones se comporte comme une régression à noyau :
```
f(x) ≈ f(x; θ₀) + ∇_θ f(x; θ₀)^T (θ - θ₀)
```

Le noyau NTK est :
```
K(x, x') = ∇_θ f(x; θ₀)^T ∇_θ f(x'; θ₀)
```

#### Implications
- Explique pourquoi les réseaux larges généralisent
- Mais ne capture pas tout le comportement des réseaux finis

### 5.5 Lottery Ticket Hypothesis

#### Énoncé
Un réseau dense contient un sous-réseau sparse (le "ticket gagnant") qui, entraîné seul depuis l'initialisation, atteint la même performance.

#### Implications
- La surparamétrisation aide à trouver de bonnes initialisations
- La structure sparse finale est ce qui compte

---

## 6. Validation et évaluation

### 6.1 Protocoles de validation

#### Hold-out
```
Données → [Train 60%] [Val 20%] [Test 20%]
```
Simple mais variance élevée si données limitées.

#### K-fold Cross-Validation
1. Diviser en K plis
2. Pour k = 1..K : entraîner sur K-1, évaluer sur 1
3. Moyenner les scores

#### Nested CV
- Boucle externe : estimation de l'erreur de généralisation
- Boucle interne : sélection des hyperparamètres

### 6.2 Métriques

#### Régression
- MSE, RMSE, MAE
- R² (coefficient de détermination)
- MAPE (Mean Absolute Percentage Error)

#### Classification
- Accuracy, Precision, Recall, F1
- AUC-ROC, AUC-PR
- Log-loss (cross-entropy)

### 6.3 Diagnostic

| Symptôme | Diagnostic | Action |
|----------|------------|--------|
| Train loss élevée | Underfitting | Augmenter capacité |
| Train ≈ Val loss, élevées | Underfitting | Augmenter capacité |
| Train << Val loss | Overfitting | Régulariser |
| Val loss oscille | Learning rate trop grand | Réduire η |

---

## Conclusion

La généralisation est le Graal de l'apprentissage automatique :

1. **Biais-variance** : compromis fondamental entre simplicité et flexibilité
2. **Théorie VC/Rademacher** : bornes formelles mais souvent pessimistes
3. **Régularisation** : contrôle explicite de la complexité
4. **Deep learning** : défie la théorie classique, nouveaux phénomènes (double descent, biais implicite)

La compréhension de ces concepts guide le design des modèles, le choix des hyperparamètres et le diagnostic des problèmes de performance.
